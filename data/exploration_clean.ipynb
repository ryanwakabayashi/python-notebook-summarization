{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# This notebook explores the use of Meta Llama 3 8B Instruct model for summarizing code snippets.\n",
    "\n",
    "Prompt engineering was accomplished in this notebook and a small set of 3500 rows was saved as data_cleaned.parquet."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dce0fb1b96dc0a2e"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "             filename                                      notebook_data  \\\n0       1000010.ipynb  import pandas as pd\\nimport seaborn as sns\\nim...   \n1       1000014.ipynb  # This Python 3 environment comes with many he...   \n2       1000018.ipynb  # This Python 3 environment comes with many he...   \n3       1000025.ipynb  As the name says, all I am doing here is clean...   \n4       1000028.ipynb  # This Python 3 environment comes with many he...   \n...               ...                                                ...   \n144390   999966.ipynb  # Overview\\nAll programmer know the basic cycl...   \n144391   999968.ipynb  Learning \\n# This Python 3 environment comes w...   \n144392   999980.ipynb  # Mean-Encoding High Cardinality Categoricals ...   \n144393   999988.ipynb  **<h1>LEAF CLASSIFICATION</h1>**\\n# Package Im...   \n144394   999995.ipynb  # Mean-Encoding High Cardinality Categoricals ...   \n\n        line_count  char_count  \n0               44        2895  \n1               41        1268  \n2               58        3119  \n3              122        6354  \n4               22         925  \n...            ...         ...  \n144390          79        4877  \n144391          42        1567  \n144392         128        5854  \n144393          91        3590  \n144394         128        5854  \n\n[144395 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>filename</th>\n      <th>notebook_data</th>\n      <th>line_count</th>\n      <th>char_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1000010.ipynb</td>\n      <td>import pandas as pd\\nimport seaborn as sns\\nim...</td>\n      <td>44</td>\n      <td>2895</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1000014.ipynb</td>\n      <td># This Python 3 environment comes with many he...</td>\n      <td>41</td>\n      <td>1268</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1000018.ipynb</td>\n      <td># This Python 3 environment comes with many he...</td>\n      <td>58</td>\n      <td>3119</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1000025.ipynb</td>\n      <td>As the name says, all I am doing here is clean...</td>\n      <td>122</td>\n      <td>6354</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1000028.ipynb</td>\n      <td># This Python 3 environment comes with many he...</td>\n      <td>22</td>\n      <td>925</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>144390</th>\n      <td>999966.ipynb</td>\n      <td># Overview\\nAll programmer know the basic cycl...</td>\n      <td>79</td>\n      <td>4877</td>\n    </tr>\n    <tr>\n      <th>144391</th>\n      <td>999968.ipynb</td>\n      <td>Learning \\n# This Python 3 environment comes w...</td>\n      <td>42</td>\n      <td>1567</td>\n    </tr>\n    <tr>\n      <th>144392</th>\n      <td>999980.ipynb</td>\n      <td># Mean-Encoding High Cardinality Categoricals ...</td>\n      <td>128</td>\n      <td>5854</td>\n    </tr>\n    <tr>\n      <th>144393</th>\n      <td>999988.ipynb</td>\n      <td>**&lt;h1&gt;LEAF CLASSIFICATION&lt;/h1&gt;**\\n# Package Im...</td>\n      <td>91</td>\n      <td>3590</td>\n    </tr>\n    <tr>\n      <th>144394</th>\n      <td>999995.ipynb</td>\n      <td># Mean-Encoding High Cardinality Categoricals ...</td>\n      <td>128</td>\n      <td>5854</td>\n    </tr>\n  </tbody>\n</table>\n<p>144395 rows Ã— 4 columns</p>\n</div>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet('data_filtered.parquet', engine='pyarrow')\n",
    "df = df.reset_index(drop=True)\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-04T18:25:15.508608700Z",
     "start_time": "2024-05-04T18:25:13.410488900Z"
    }
   },
   "id": "4703d9034776fcc1",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "base_model = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "quant_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_use_double_quant=False)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "    \n",
    "compute_dtype = getattr(torch, \"float16\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-04T18:25:18.764386600Z",
     "start_time": "2024-05-04T18:25:15.510645800Z"
    }
   },
   "id": "d52b9c1f483695da",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "932e9c8f27f142d592113143662a8572"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(base_model, device_map=\"auto\", torch_dtype=compute_dtype, quantization_config=quant_config, low_cpu_mem_usage=True, temperature=0.1, do_sample=True)\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-04T18:25:32.906552800Z",
     "start_time": "2024-05-04T18:25:18.761880400Z"
    }
   },
   "id": "7f118dda6c79b8f0",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def generate_response(prompt):\n",
    "    chat = [{'content': prompt, 'role': 'user'}]\n",
    "    chat_tokens = tokenizer.apply_chat_template(chat, tokenize=True, add_generation_prompt=True, return_tensors='pt').to(model.device)\n",
    "\n",
    "    new_chat_tokens = model.generate(chat_tokens, do_sample=False, max_new_tokens=512, pad_token_id=tokenizer.pad_token_id)\n",
    "    new_chat_str = tokenizer.decode(new_chat_tokens[0])\n",
    "    return new_chat_str"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-04T18:25:32.911158900Z",
     "start_time": "2024-05-04T18:25:32.907556100Z"
    }
   },
   "id": "4c6cacb9f2ef3236",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryanw\\anaconda3\\envs\\llama3-test\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ryanw\\anaconda3\\envs\\llama3-test\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ryanw\\anaconda3\\envs\\llama3-test\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:671: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Summarize the following code in two to three sentences. Focus on the intentions of the code, emphasize the type of data being handled, do not give an introductory clause. do nothing else.\n",
      "\n",
      "chunksize = 100000\n",
      "df_chunks = []\n",
      "\n",
      "for df in pd.read_csv(\n",
      "    crimes_raw_file_path, parse_dates=[2], index_col=[2],\n",
      "    chunksize=chunksize, iterator=True, low_memory=True\n",
      "):\n",
      "    df = df['2016-01-01':'2016-12-31']\n",
      "    df_chunks.append(df)\n",
      "\n",
      "crimes2016 = pd.concat(df_chunks)\n",
      "\n",
      "# Serialising & compressing the data\n",
      "crimes2016.sort_index(ascending=True).to_csv(crimes_file_path)\n",
      "\n",
      "crimes2016.head()<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "The code reads a large CSV file in chunks, filters the data to a specific date range, and concatenates the chunks into a single DataFrame. The intention is to process and store the data in a compressed format, likely for further analysis or storage. The data being handled is crime-related information, with dates parsed as datetime objects.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "# prompt engineering\n",
    "\n",
    "# task = \"Summarize the following code in two to three sentences. Focus on the intentions of the code, emphasize the type of data being handled, and do nothing else.\\n\\n\"\n",
    "\n",
    "task = \"Summarize the following code in two to three sentences. Focus on the intentions of the code, emphasize the type of data being handled, do not give an introductory clause. do nothing else.\\n\\n\"\n",
    "\n",
    "# task = \"Summarize the following code in two to three sentences. Focus on the intentions of the code, detail the data being handled, do not give an introductory clause. do nothing else\"\n",
    "\n",
    "input_text = '''chunksize = 100000\n",
    "df_chunks = []\n",
    "\n",
    "for df in pd.read_csv(\n",
    "    crimes_raw_file_path, parse_dates=[2], index_col=[2],\n",
    "    chunksize=chunksize, iterator=True, low_memory=True\n",
    "):\n",
    "    df = df['2016-01-01':'2016-12-31']\n",
    "    df_chunks.append(df)\n",
    "\n",
    "crimes2016 = pd.concat(df_chunks)\n",
    "\n",
    "# Serialising & compressing the data\n",
    "crimes2016.sort_index(ascending=True).to_csv(crimes_file_path)\n",
    "\n",
    "crimes2016.head()'''\n",
    "print(generate_response(task + input_text))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-04T18:25:36.663419100Z",
     "start_time": "2024-05-04T18:25:32.913158700Z"
    }
   },
   "id": "6c8e8faf01a1c1d0",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nSummarize the following code in two to three sentences. Focus on the intentions of the code, emphasize the type of data being handled, and do nothing else. \\n\\nchunksize = 100000\\ndf_chunks = []\\n\\nfor df in pd.read_csv(\\n    crimes_raw_file_path, parse_dates=[2], index_col=[2],\\n    chunksize=chunksize, iterator=True, low_memory=True\\n):\\n    df = df['2016-01-01':'2016-12-31']\\n    df_chunks.append(df)\\n\\ncrimes2016 = pd.concat(df_chunks)\\n\\n# Serialising & compressing the data\\ncrimes2016.sort_index(ascending=True).to_csv(crimes_file_path)\\n\\ncrimes2016.head()<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nThe code intends to read a large CSV file, process it in chunks, and then concatenate the chunks into a single DataFrame. The data being handled is crime records from 2016, with dates parsed and used as the index. The processed data is then written to a new CSV file.<|eot_id|>\""
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = generate_response('''Summarize the following code in two to three sentences. Focus on the intentions of the code, emphasize the type of data being handled, and do nothing else. \n",
    "\n",
    "chunksize = 100000\n",
    "df_chunks = []\n",
    "\n",
    "for df in pd.read_csv(\n",
    "    crimes_raw_file_path, parse_dates=[2], index_col=[2],\n",
    "    chunksize=chunksize, iterator=True, low_memory=True\n",
    "):\n",
    "    df = df['2016-01-01':'2016-12-31']\n",
    "    df_chunks.append(df)\n",
    "\n",
    "crimes2016 = pd.concat(df_chunks)\n",
    "\n",
    "# Serialising & compressing the data\n",
    "crimes2016.sort_index(ascending=True).to_csv(crimes_file_path)\n",
    "\n",
    "crimes2016.head()''')\n",
    "response"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-04T18:25:39.664889700Z",
     "start_time": "2024-05-04T18:25:36.662417400Z"
    }
   },
   "id": "89dbfdff65e33834",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(\"Summarize the following code in two to three sentences. Focus on the intentions of the code, emphasize the type of data being handled, and do nothing else. \\n\\nchunksize = 100000\\ndf_chunks = []\\n\\nfor df in pd.read_csv(\\n    crimes_raw_file_path, parse_dates=[2], index_col=[2],\\n    chunksize=chunksize, iterator=True, low_memory=True\\n):\\n    df = df['2016-01-01':'2016-12-31']\\n    df_chunks.append(df)\\n\\ncrimes2016 = pd.concat(df_chunks)\\n\\n# Serialising & compressing the data\\ncrimes2016.sort_index(ascending=True).to_csv(crimes_file_path)\\n\\ncrimes2016.head()\",\n 'The code intends to read a large CSV file, process it in chunks, and then concatenate the chunks into a single DataFrame. The data being handled is crime records from 2016, with dates parsed and used as the index. The processed data is then written to a new CSV file.')"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# understand parsing the response\n",
    "parts = response.split(\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n\")\n",
    "parts = parts[1].split(\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\")\n",
    "prompt = parts[0].strip()\n",
    "answer = parts[1].split(\"<|eot_id|>\")[0].strip()\n",
    "prompt, answer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-04T18:25:39.668102300Z",
     "start_time": "2024-05-04T18:25:39.663889400Z"
    }
   },
   "id": "38c261f7dc2a7002",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# copy to make a small scale dataframe\n",
    "df_test = df[:3500].copy() # 3500 samples is approximately 3 hrs and 14 min of runtime on rtx 4090"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-04T18:25:39.670669Z",
     "start_time": "2024-05-04T18:25:39.668102300Z"
    }
   },
   "id": "8fd61502fc4232f5",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def summarize(row):\n",
    "    response = generate_response(task + row['notebook_data'])\n",
    "    parts = response.split(\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n\")\n",
    "    parts = parts[1].split(\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\")\n",
    "    prompt = parts[0].strip()\n",
    "    answer = parts[1].split(\"<|eot_id|>\")[0].strip()\n",
    "    return pd.Series([prompt, answer], index=['question', 'answer'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-04T18:25:39.676175100Z",
     "start_time": "2024-05-04T18:25:39.671665500Z"
    }
   },
   "id": "226a9fd5826ffca",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "           filename                                      notebook_data  \\\n0     1000010.ipynb  import pandas as pd\\nimport seaborn as sns\\nim...   \n1     1000014.ipynb  # This Python 3 environment comes with many he...   \n2     1000018.ipynb  # This Python 3 environment comes with many he...   \n3     1000025.ipynb  As the name says, all I am doing here is clean...   \n4     1000028.ipynb  # This Python 3 environment comes with many he...   \n...             ...                                                ...   \n3495  1025649.ipynb  \\nthis is a combination of my learning from ka...   \n3496  1025658.ipynb  #Interacting with Data\\nAn interactive visuali...   \n3497  1025660.ipynb  \\nthis is a combination of my learning from ka...   \n3498  1025661.ipynb  **Exploring Homicide Reports 1980-2014!**\\n\\nH...   \n3499  1025663.ipynb  ## Introduction ##\\n\\nThis is a simple dataset...   \n\n      line_count  char_count  \n0             44        2895  \n1             41        1268  \n2             58        3119  \n3            122        6354  \n4             22         925  \n...          ...         ...  \n3495         243        9120  \n3496          62        2567  \n3497         283       10752  \n3498         167        7658  \n3499          21         588  \n\n[3500 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>filename</th>\n      <th>notebook_data</th>\n      <th>line_count</th>\n      <th>char_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1000010.ipynb</td>\n      <td>import pandas as pd\\nimport seaborn as sns\\nim...</td>\n      <td>44</td>\n      <td>2895</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1000014.ipynb</td>\n      <td># This Python 3 environment comes with many he...</td>\n      <td>41</td>\n      <td>1268</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1000018.ipynb</td>\n      <td># This Python 3 environment comes with many he...</td>\n      <td>58</td>\n      <td>3119</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1000025.ipynb</td>\n      <td>As the name says, all I am doing here is clean...</td>\n      <td>122</td>\n      <td>6354</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1000028.ipynb</td>\n      <td># This Python 3 environment comes with many he...</td>\n      <td>22</td>\n      <td>925</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3495</th>\n      <td>1025649.ipynb</td>\n      <td>\\nthis is a combination of my learning from ka...</td>\n      <td>243</td>\n      <td>9120</td>\n    </tr>\n    <tr>\n      <th>3496</th>\n      <td>1025658.ipynb</td>\n      <td>#Interacting with Data\\nAn interactive visuali...</td>\n      <td>62</td>\n      <td>2567</td>\n    </tr>\n    <tr>\n      <th>3497</th>\n      <td>1025660.ipynb</td>\n      <td>\\nthis is a combination of my learning from ka...</td>\n      <td>283</td>\n      <td>10752</td>\n    </tr>\n    <tr>\n      <th>3498</th>\n      <td>1025661.ipynb</td>\n      <td>**Exploring Homicide Reports 1980-2014!**\\n\\nH...</td>\n      <td>167</td>\n      <td>7658</td>\n    </tr>\n    <tr>\n      <th>3499</th>\n      <td>1025663.ipynb</td>\n      <td>## Introduction ##\\n\\nThis is a simple dataset...</td>\n      <td>21</td>\n      <td>588</td>\n    </tr>\n  </tbody>\n</table>\n<p>3500 rows Ã— 4 columns</p>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-04T18:25:39.682031500Z",
     "start_time": "2024-05-04T18:25:39.675177200Z"
    }
   },
   "id": "4d97ba077f25d0b4",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryanw\\anaconda3\\envs\\llama3-test\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ryanw\\anaconda3\\envs\\llama3-test\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2h 51min 21s\n",
      "Wall time: 3h 19min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_test[['question', 'answer']] = df_test.apply(summarize, axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-04T21:44:57.496498300Z",
     "start_time": "2024-05-04T18:25:39.682031500Z"
    }
   },
   "id": "6ed157d30c51c626",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "           filename                                      notebook_data  \\\n0     1000010.ipynb  import pandas as pd\\nimport seaborn as sns\\nim...   \n1     1000014.ipynb  # This Python 3 environment comes with many he...   \n2     1000018.ipynb  # This Python 3 environment comes with many he...   \n3     1000025.ipynb  As the name says, all I am doing here is clean...   \n4     1000028.ipynb  # This Python 3 environment comes with many he...   \n...             ...                                                ...   \n3495  1025649.ipynb  \\nthis is a combination of my learning from ka...   \n3496  1025658.ipynb  #Interacting with Data\\nAn interactive visuali...   \n3497  1025660.ipynb  \\nthis is a combination of my learning from ka...   \n3498  1025661.ipynb  **Exploring Homicide Reports 1980-2014!**\\n\\nH...   \n3499  1025663.ipynb  ## Introduction ##\\n\\nThis is a simple dataset...   \n\n      line_count  char_count  \\\n0             44        2895   \n1             41        1268   \n2             58        3119   \n3            122        6354   \n4             22         925   \n...          ...         ...   \n3495         243        9120   \n3496          62        2567   \n3497         283       10752   \n3498         167        7658   \n3499          21         588   \n\n                                               question  \\\n0     Summarize the following code in two to three s...   \n1     Summarize the following code in two to three s...   \n2     Summarize the following code in two to three s...   \n3     Summarize the following code in two to three s...   \n4     Summarize the following code in two to three s...   \n...                                                 ...   \n3495  Summarize the following code in two to three s...   \n3496  Summarize the following code in two to three s...   \n3497  Summarize the following code in two to three s...   \n3498  Summarize the following code in two to three s...   \n3499  Summarize the following code in two to three s...   \n\n                                                 answer  \n0     The code intends to analyze and visualize happ...  \n1     The code is intended to train a linear model u...  \n2     The code intends to analyze and classify data ...  \n3     The code is focused on cleaning and preprocess...  \n4     The code intends to load and process data from...  \n...                                                 ...  \n3495  The code aims to predict who survived the sink...  \n3496  The code is designed to create interactive vis...  \n3497  The code aims to predict who survived the sink...  \n3498  The code explores homicide reports from 1980 t...  \n3499  The code reads in a CSV dataset named \"crime.c...  \n\n[3500 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>filename</th>\n      <th>notebook_data</th>\n      <th>line_count</th>\n      <th>char_count</th>\n      <th>question</th>\n      <th>answer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1000010.ipynb</td>\n      <td>import pandas as pd\\nimport seaborn as sns\\nim...</td>\n      <td>44</td>\n      <td>2895</td>\n      <td>Summarize the following code in two to three s...</td>\n      <td>The code intends to analyze and visualize happ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1000014.ipynb</td>\n      <td># This Python 3 environment comes with many he...</td>\n      <td>41</td>\n      <td>1268</td>\n      <td>Summarize the following code in two to three s...</td>\n      <td>The code is intended to train a linear model u...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1000018.ipynb</td>\n      <td># This Python 3 environment comes with many he...</td>\n      <td>58</td>\n      <td>3119</td>\n      <td>Summarize the following code in two to three s...</td>\n      <td>The code intends to analyze and classify data ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1000025.ipynb</td>\n      <td>As the name says, all I am doing here is clean...</td>\n      <td>122</td>\n      <td>6354</td>\n      <td>Summarize the following code in two to three s...</td>\n      <td>The code is focused on cleaning and preprocess...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1000028.ipynb</td>\n      <td># This Python 3 environment comes with many he...</td>\n      <td>22</td>\n      <td>925</td>\n      <td>Summarize the following code in two to three s...</td>\n      <td>The code intends to load and process data from...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3495</th>\n      <td>1025649.ipynb</td>\n      <td>\\nthis is a combination of my learning from ka...</td>\n      <td>243</td>\n      <td>9120</td>\n      <td>Summarize the following code in two to three s...</td>\n      <td>The code aims to predict who survived the sink...</td>\n    </tr>\n    <tr>\n      <th>3496</th>\n      <td>1025658.ipynb</td>\n      <td>#Interacting with Data\\nAn interactive visuali...</td>\n      <td>62</td>\n      <td>2567</td>\n      <td>Summarize the following code in two to three s...</td>\n      <td>The code is designed to create interactive vis...</td>\n    </tr>\n    <tr>\n      <th>3497</th>\n      <td>1025660.ipynb</td>\n      <td>\\nthis is a combination of my learning from ka...</td>\n      <td>283</td>\n      <td>10752</td>\n      <td>Summarize the following code in two to three s...</td>\n      <td>The code aims to predict who survived the sink...</td>\n    </tr>\n    <tr>\n      <th>3498</th>\n      <td>1025661.ipynb</td>\n      <td>**Exploring Homicide Reports 1980-2014!**\\n\\nH...</td>\n      <td>167</td>\n      <td>7658</td>\n      <td>Summarize the following code in two to three s...</td>\n      <td>The code explores homicide reports from 1980 t...</td>\n    </tr>\n    <tr>\n      <th>3499</th>\n      <td>1025663.ipynb</td>\n      <td>## Introduction ##\\n\\nThis is a simple dataset...</td>\n      <td>21</td>\n      <td>588</td>\n      <td>Summarize the following code in two to three s...</td>\n      <td>The code reads in a CSV dataset named \"crime.c...</td>\n    </tr>\n  </tbody>\n</table>\n<p>3500 rows Ã— 6 columns</p>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-04T21:44:57.503858100Z",
     "start_time": "2024-05-04T21:44:57.491863Z"
    }
   },
   "id": "899c75e33df29b5c",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "\"Summarize the following code in two to three sentences. Focus on the intentions of the code, emphasize the type of data being handled, do not give an introductory clause. do nothing else.\\n\\nimport pandas as pd\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n%matplotlib inline\\nwh2015 = pd.read_csv('../input/2015.csv')\\nwh2015.info()\\nwh2016 = pd.read_csv('../input/2016.csv')\\nwh2016.info()\\n#max happiness score for 2015\\nwh2015[wh2015['Happiness Score'] == wh2015['Happiness Score'].max()][['Country','Happiness Score', 'Happiness Rank']]\\n#max happiness score for 2015\\nwh2016[wh2016['Happiness Score'] == wh2016['Happiness Score'].max()][['Country','Happiness Score', 'Happiness Rank']]\\n#min happiness score for 2015\\nwh2015[wh2015['Happiness Score'] == wh2015['Happiness Score'].min()][['Country','Happiness Score', 'Happiness Rank']]\\n#min happiness score for 2016\\nwh2016[wh2016['Happiness Score'] == wh2016['Happiness Score'].min()][['Country','Happiness Score', 'Happiness Rank']]\\n#factors for most happy country - Switzerland 2015\\nwh2015[wh2015['Country'] == 'Switzerland'][['Economy (GDP per Capita)', 'Family',\\n       'Health (Life Expectancy)', 'Freedom',\\n       'Trust (Government Corruption)', 'Generosity', 'Happiness Score', 'Happiness Rank']]\\n#factors for- Switzerland 2016\\nwh2016[wh2016['Country'] == 'Switzerland'][['Economy (GDP per Capita)', 'Family',\\n       'Health (Life Expectancy)', 'Freedom',\\n       'Trust (Government Corruption)', 'Generosity', 'Happiness Score', 'Happiness Rank']]\\n#factors for most happy country - Denmark 2016\\nwh2016[wh2016['Country'] == 'Denmark'][['Economy (GDP per Capita)', 'Family',\\n       'Health (Life Expectancy)', 'Freedom',\\n       'Trust (Government Corruption)', 'Generosity', 'Happiness Score', 'Happiness Rank']]\\n#factors for- Denmark 2015\\nwh2015[wh2015['Country'] == 'Denmark'][['Economy (GDP per Capita)', 'Family',\\n       'Health (Life Expectancy)', 'Freedom',\\n       'Trust (Government Corruption)', 'Generosity', 'Happiness Score', 'Happiness Rank']]\\n#increase in family and generosity seems major factor in affecting overall happiness score\\n#plot for Happiness Score vs all factors to judge if Family and generosity play major role in 2015\\nsns.pairplot(data = wh2015.drop(['Country', 'Region', 'Happiness Rank', 'Standard Error', 'Dystopia Residual'], axis = 1))\\n#plot for Happiness Score vs all factors to judge if Family and generosity play major role in 2015\\nsns.pairplot(data = wh2016.drop(['Country', 'Region', 'Happiness Rank', 'Dystopia Residual', 'Upper Confidence Interval', 'Lower Confidence Interval'], axis = 1))\\n#Region wise happiness scores for 2015 show western europe to be relatively happier than others and sub saharan africa as least happiest\\nplt.figure(figsize=(12,6))\\nsns.swarmplot('Region', 'Happiness Score', data = wh2015)\\nplt.xticks(rotation = 60)\\n#Region wise happiness scores for 2016 show western europe to be relatively happier than others and sub saharan africa as least happiest\\nplt.figure(figsize=(12,6))\\nsns.swarmplot('Region', 'Happiness Score', data = wh2016)\\nplt.xticks(rotation = 60)\""
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['question'][0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-04T21:44:57.509920700Z",
     "start_time": "2024-05-04T21:44:57.501589900Z"
    }
   },
   "id": "bfcee1fba32ce48f",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'Summarize the following code in two to three sentences. Focus on the intentions of the code, emphasize the type of data being handled, do not give an introductory clause. do nothing else.\\n\\n# This Python 3 environment comes with many helpful analytics libraries installed\\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\\n# For example, here\\'s several helpful packages to load in \\n\\nimport numpy as np # linear algebra\\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\\n\\nimport scipy\\n\\nfrom sklearn.feature_extraction.text import HashingVectorizer\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\n\\nfrom sklearn.neighbors import KNeighborsClassifier\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.metrics import log_loss\\nfrom sklearn.model_selection import train_test_split\\n\\n# Input data files are available in the \"../input/\" directory.\\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\\n\\nfrom subprocess import check_output\\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\\n\\n# Any results you write to the current directory are saved as output.\\ndf=pd.read_json(\"../input/train.json\")\\ndf[\\'priceperbed\\']=(df[\\'price\\'].clip(upper=7000)/df[\\'bedrooms\\'].clip(lower=1))\\ndf[\\'created\\']=df[\\'created\\'].astype(np.datetime64)\\ndf[\\'created_day\\']=np.array(df.created.values, dtype=\\'datetime64[D]\\').astype(np.float32)%7\\ndf[\\'created_week\\']=np.array(df.created.values, dtype=\\'datetime64[W]\\').astype(np.float32)\\ndf[\\'created_hour\\']=np.array(df.created.values, dtype=\\'datetime64[h]\\').astype(np.float32)%24\\ndf[\\'desc_count\\']=df.description.apply(lambda x: len(x.split())).clip(upper=150)\\ndf[\\'features_count\\']=df.features.apply(lambda x: len(x))\\ndf[\\'photos_count\\']=df.photos.apply(lambda x: len(x))\\nfeature_list=[\\'no fee\\', \\'hardwood floors\\', \\'laundry in building\\']\\ndf[\\'features\\']=df[\\'features\\'].apply(lambda x: list(map(str.lower, x)))\\nfor feature in feature_list:\\n        df[feature]=df[\\'features\\'].apply(lambda x: feature in x)\\ncols=[\\'price\\', \\'bathrooms\\', \\'bedrooms\\', \\'priceperbed\\',\\'created_hour\\',  \\'desc_count\\', \\n      \\'photos_count\\', \\'features_count\\', \\'no fee\\', \\'hardwood floors\\', \\'laundry in building\\']        \\ndf_tv, df_test = train_test_split(df, random_state=0)\\ndf_train, df_val = train_test_split(df_tv, random_state=0)\\nvectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\\n                                 stop_words=\\'english\\')\\nvectorizer.fit(df.description.values)\\nX_train = vectorizer.transform(df_train.description)\\n#X_train_dense=scipy.sparse.csr_matrix(df_train[cols].values, dtype=np.float64)\\n#X_train=scipy.sparse.hstack([X_train, X_train_dense])\\nX_val = vectorizer.transform(df_val.description)\\n#X_val_dense=scipy.sparse.csr_matrix(df_val[cols].values, dtype=np.float64)\\n#X_val=scipy.sparse.hstack([X_val, X_val_dense])\\nfrom sklearn.neighbors import KNeighborsClassifier\\n#clf=RandomForestClassifier(max_depth=30, n_estimators=1000, min_samples_split=10)\\nclf=KNeighborsClassifier(n_neighbors=10)\\nclf.fit(X_train, df_train[\\'interest_level\\'])\\ny_pred=clf.predict_proba(X_train)\\nscore=log_loss(df_train[\\'interest_level\\'].values, y_pred)\\ny_pred=clf.predict_proba(X_val)\\nscore2=log_loss(df_val[\\'interest_level\\'].values, y_pred)\\nprint(\"%.6f %.6f\"%(score, score2))'"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['question'][2]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-04T21:44:57.549734200Z",
     "start_time": "2024-05-04T21:44:57.509920700Z"
    }
   },
   "id": "b82bb6810f9b1e98",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'The code is focused on cleaning and preprocessing the data in the \"houseprice\" dataset, specifically replacing missing values in the training data. The data being handled is a dataset of house prices, with various features such as LotFrontage, Alley, MasVnrType, and others. The goal is to prepare the data for further exploration and feature engineering, with the intention of building a machine learning model.'"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['answer'][3]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-04T21:44:57.550255300Z",
     "start_time": "2024-05-04T21:44:57.516270600Z"
    }
   },
   "id": "5a24ea3b84d0013c",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_test.to_parquet('data_cleaned.parquet', engine='pyarrow')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-04T21:44:57.677384800Z",
     "start_time": "2024-05-04T21:44:57.521375500Z"
    }
   },
   "id": "7f8c605f13379490",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# df_clean = pd.read_parquet('data_cleaned.parquet', engine='pyarrow')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-04T21:44:57.677384800Z",
     "start_time": "2024-05-04T21:44:57.602941100Z"
    }
   },
   "id": "707468c998b47da8",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-04T21:44:57.677915200Z",
     "start_time": "2024-05-04T21:44:57.606656800Z"
    }
   },
   "id": "6f680b7ba72aa47b",
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
