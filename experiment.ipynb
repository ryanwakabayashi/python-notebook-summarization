{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-24T15:07:36.684032300Z",
     "start_time": "2024-04-24T15:07:26.956526800Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7fd73311c0104cc6b27156449f8b00ae"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "model_path = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "revision = \"refs/pr/4\"\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, revision=revision)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, revision=revision, device_map=\"auto\", torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def generate_response(prompt):\n",
    "    chat = [{'content': prompt, 'role': 'user'}]\n",
    "    chat_tokens = tokenizer.apply_chat_template(chat, tokenize=True, add_generation_prompt=True, return_tensors='pt').to(model.device)\n",
    "\n",
    "    new_chat_tokens = model.generate(chat_tokens, do_sample=False, max_new_tokens=512, pad_token_id=tokenizer.pad_token_id)\n",
    "    new_chat_str = tokenizer.decode(new_chat_tokens[0])\n",
    "    return new_chat_str"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T14:33:23.681431800Z",
     "start_time": "2024-04-24T14:33:23.676474Z"
    }
   },
   "id": "5646e69f8500e70b",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryanw\\anaconda3\\envs\\llama3-test\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:671: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Think step by step and then provide a two or three sentence summary of what the code is doing for an audience who may not be familiar with machine learning. Focus on the problem the authors' are trying to solve and do nothing else. \n",
      "\n",
      "chunksize = 100000\n",
      "df_chunks = []\n",
      "\n",
      "for df in pd.read_csv(\n",
      "    crimes_raw_file_path, parse_dates=[2], index_col=[2],\n",
      "    chunksize=chunksize, iterator=True, low_memory=True\n",
      "):\n",
      "    df = df['2016-01-01':'2016-12-31']\n",
      "    df_chunks.append(df)\n",
      "    \n",
      "crimes2016 = pd.concat(df_chunks)\n",
      "\n",
      "# Serialising & compressing the data\n",
      "crimes2016.sort_index(ascending=True).to_csv(crimes_file_path)\n",
      "\n",
      "crimes2016.head()<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Here's a step-by-step breakdown of the code:\n",
      "\n",
      "1. The code reads a large CSV file containing crime data in chunks of 100,000 rows at a time.\n",
      "2. It filters each chunk to only include data from 2016 and appends it to a list of chunks.\n",
      "3. Once all chunks have been processed, the code concatenates the filtered chunks into a single DataFrame.\n",
      "4. The resulting DataFrame is then sorted by date and written to a new CSV file.\n",
      "\n",
      "In summary, this code is trying to solve the problem of processing and storing a large dataset of crime data from 2016, by reading it in chunks, filtering it to only include 2016 data, and then writing it to a new file in a sorted and compressed format.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(generate_response('''Think step by step and then provide a two or three sentence summary of what the code is doing for an audience who may not be familiar with machine learning. Focus on the problem the authors' are trying to solve and do nothing else. \n",
    "\n",
    "chunksize = 100000\n",
    "df_chunks = []\n",
    "\n",
    "for df in pd.read_csv(\n",
    "    crimes_raw_file_path, parse_dates=[2], index_col=[2],\n",
    "    chunksize=chunksize, iterator=True, low_memory=True\n",
    "):\n",
    "    df = df['2016-01-01':'2016-12-31']\n",
    "    df_chunks.append(df)\n",
    "    \n",
    "crimes2016 = pd.concat(df_chunks)\n",
    "\n",
    "# Serialising & compressing the data\n",
    "crimes2016.sort_index(ascending=True).to_csv(crimes_file_path)\n",
    "\n",
    "crimes2016.head()'''))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T14:33:28.923285100Z",
     "start_time": "2024-04-24T14:33:23.680432900Z"
    }
   },
   "id": "529eb62e997bc425",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4852d681f3af7b96"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
